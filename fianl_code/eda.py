# -*- coding: utf-8 -*-
"""EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15xuhqqob5ULig17xTuAyGkfEkY1Yqp7F
"""

# Commented out IPython magic to ensure Python compatibility.
# #@title  <-- Run Me: (Hidden) Installing Spark
# %%bash 
# 
# ## Setup Spark on Colab
# pip install -q pyspark
# apt-get -qq install -y openjdk-8-jdk-headless
# 
# ## Setup port-forwarding
# 
# # Download ngrok
# wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
# # Unload ngrok
# unzip -q ngrok-stable-linux-amd64.zip

@title  <-- Run Me: (Hidden) Environment Variable Setup
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

import pyspark 
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf, SQLContext 

# Set configuration scope to be local and use port 4050
config_scope = SparkConf().set("spark.ui.port", "4050")

# Create the connection to a Spark cluster
sc = pyspark.SparkContext(conf = config_scope)

# Create a session to programmatically build Spark RDD, DataFrame and DataSet
spark = SparkSession.builder.getOrCreate()

# connect with Google Drive
from google.colab import drive
drive.mount('/content/drive')



# Use the data from data processing
# unzip zip file
!unzip /content/drive/MyDrive/stat480/model_df.zip

data = spark.read.csv("model_df.csv",inferSchema=True,header=True)

data.createOrReplaceTempView("df")
query = """
SELECT TIMESTAMP(date) AS ds,  station_id, diff AS y  FROM 
(SELECT date|| ' ' || hour AS date,  station_id, diff
FROM df)
"""
time_series_df = spark.sql(query)

time_series_df.select("y").summary().show()

import matplotlib
import matplotlib.pyplot as plt
import numpy as np

from datetime import datetime
import pandas as pd

from fbprophet import Prophet

matplotlib.rcParams['figure.figsize'] = (10, 8)
matplotlib.rcParams['axes.grid'] = False

y_histogram = time_series_df.select('y').rdd.flatMap(lambda x: x).histogram(20)
pd.DataFrame(
    list(zip(*y_histogram)), 
    columns=['bin', 'frequency']
).set_index(
    'bin'
).plot(kind='bar');

station_df = pd.read_csv('london_stations.csv')
model_df = pd.read_csv("model_df.csv")

plot_df = model_df.merge(station_df, how = 'left', on = 'station_id')
sub_plot = plot_df.loc[(plot_df['date'] == '2016-12-28') & (plot_df['hour']==12)]

import folium

map = folium.Map(location=[station_df.latitude.mean(), station_df.longitude.mean()], zoom_start=14, control_scale=True)
for index, location_info in station_df.iterrows():
    folium.Marker([location_info["latitude"], location_info["longitude"]], popup=location_info["station_name"], icon=folium.Icon(color='red')).add_to(map)
map

map = folium.Map(location=[sub_plot.latitude.mean(), sub_plot.longitude.mean()], zoom_start=14, control_scale=True)
for index, location_info in sub_plot.iterrows():
    if location_info['diff'] > 0:
        folium.Marker([location_info["latitude"], location_info["longitude"]], popup=location_info["station_name_y"], icon=folium.Icon(color='red')).add_to(map)
    else:
        folium.Marker([location_info["latitude"], location_info["longitude"]], popup=location_info["station_name_y"], icon=folium.Icon(color='green')).add_to(map)
map

# check popularity of stations
time_series_df.createOrReplaceTempView("df2")
query = """
SELECT  station_id, COUNT(station_id) AS n  
FROM df2
GROUP BY  station_id
ORDER BY n DESC
"""

# Query data
spark.sql(query).show()

# subset with the top popularity stations
query = """
SELECT  ds, station_id, y 
FROM df2
WHERE station_id IN (132,194,14,74,39,251,553,732,270,356,405,737,282)
ORDER BY station_id, ds
"""

# Query data
sub_time_df = spark.sql(query)

sub_df = sub_time_df.toPandas()
plot_132 = sub_df.loc[(sub_df['station_id']==132)& (sub_df['ds']<'2017-02-01')& (sub_df['ds']>='2017-01-01')]
plot_132.set_index('ds')[['y']].plot()

plots = sub_df.loc[(sub_df['ds']<'2017-01-11')& (sub_df['ds']>='2017-01-01')]
plots.set_index('ds').groupby('station_id')['y'].plot(legend=True)