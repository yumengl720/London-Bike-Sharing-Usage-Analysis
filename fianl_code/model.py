# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iqI9pi5x64Nwy-GNKHUgNbDaKOEm4mWF
"""

# Commented out IPython magic to ensure Python compatibility.
# #@title  <-- Run Me: (Hidden) Installing Spark
# %%bash 
# 
# ## Setup Spark on Colab
# pip install -q pyspark
# apt-get -qq install -y openjdk-8-jdk-headless
# 
# ## Setup port-forwarding
# 
# # Download ngrok
# wget -q https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
# # Unload ngrok
# unzip -q ngrok-stable-linux-amd64.zip

#@title  <-- Run Me: (Hidden) Environment Variable Setup
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

import pyspark 
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf, SQLContext 

# Set configuration scope to be local and use port 4050
config_scope = SparkConf().set("spark.ui.port", "4050")

# Create the connection to a Spark cluster
sc = pyspark.SparkContext(conf = config_scope)

# Create a session to programmatically build Spark RDD, DataFrame and DataSet
spark = SparkSession.builder.getOrCreate()

# connect with Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Use the data from data processing
# unzip zip file
!unzip /content/drive/MyDrive/stat480/model_df.zip

data = spark.read.csv("model_df.csv",inferSchema=True,header=True)

data.createOrReplaceTempView("df")
query = """
SELECT TIMESTAMP(date) AS ds,  station_id, diff AS y  FROM 
(SELECT date|| ' ' || hour AS date,  station_id, diff
FROM df)
"""
time_series_df = spark.sql(query)

import pandas as pd

station_df = pd.read_csv('london_stations.csv')
model_df = pd.read_csv("model_df.csv")

plot_df = model_df.merge(station_df, how = 'left', on = 'station_id')
sub_plot = plot_df.loc[(plot_df['date'] == '2016-12-28') & (plot_df['hour']==12)]

# subset with the top popularity stations
query = """
SELECT  ds, station_id, y 
FROM df2
WHERE station_id IN (132,194,14,74,39,251,553,732,270,356,405,737,282)
ORDER BY station_id, ds
"""

# Query data
sub_time_df = spark.sql(query)

"""Time series"""

# check date range of sub_time_df
sub_time_df.agg(min("ds"), max("ds")).show()

# decide trian-test split point
(sub_time_df.filter(sub_time_df.ds < "2020-03-09")).count()/sub_time_df.count()

# train-test split
train_set = sub_time_df.filter(sub_time_df.ds < "2020-03-09")
test_set = sub_time_df.filter(sub_time_df.ds >= "2020-03-09")

# check date range of train_set
train_set.agg(min("ds"), max("ds")).show()

# check date range of test set
test_set.agg(min("ds"), max("ds")).show()

store_part = (train_set.repartition(spark.sparkContext.defaultParallelism, ['station_id'])).cache()

from pyspark.sql.types import *

result_schema =StructType([
  StructField('ds',TimestampType()),
  StructField('station_id',IntegerType()),
  StructField('y',IntegerType()),
  StructField('yhat',DoubleType()),
  StructField('yhat_upper',DoubleType()),
  StructField('yhat_lower',DoubleType()),
  StructField('trend',DoubleType()),
  StructField('yearly',DoubleType()),
  StructField('daily',DoubleType()),
  StructField('weekly',DoubleType())  
  ])

from pyspark.sql.functions import pandas_udf, PandasUDFType
from fbprophet import Prophet

@pandas_udf( result_schema, PandasUDFType.GROUPED_MAP )
def forecast_sales( train):

    model = Prophet(
    interval_width=0.95,
    growth='linear',
    daily_seasonality=True,
    weekly_seasonality=True,
    yearly_seasonality=True,
    seasonality_mode='multiplicative'
)

    model.fit( train )

    future_pd = model.make_future_dataframe(
    periods=4251, 
    freq='h',
    include_history=True
    )
  
    forecast_pd = model.predict( future_pd )  
  
    f_pd = forecast_pd[ ['ds','yhat', 'yhat_upper', 'yhat_lower','trend', 'yearly', 'daily','weekly'] ].set_index('ds')
    #f_pd = forecast_pd.set_index('ds')
  
    st_pd = train[['ds','station_id','y']].set_index('ds')
  
    results_pd = f_pd.join( st_pd, how='left' )
    results_pd.reset_index(level=0, inplace=True)
  
    results_pd['station_id'] = train['station_id'].iloc[0]

    return results_pd[ ['ds', 'station_id','y', 'yhat', 'yhat_upper', 'yhat_lower','trend', 'yearly', 'daily','weekly'] ]
    #return results_pd

results = (
  store_part
    .groupBy('station_id')
    .apply(forecast_sales))
results.show()
results_df = results.toPandas()
results_df = results_df.set_index('ds')

# prophet model plot
results_df.query('station_id == 132')[['y', 'yhat', 'yhat_upper','yhat_lower']][-4500:-4000].plot()

# true df with station 132 and test date range
df_132 =  sub_df.loc[(sub_df['station_id']==132) & (sub_df['ds']>='2020-03-08 21:00:00')]
df_132

# prediction df with station 132 and test date range
predict_132 = results_df.loc[(results_df['station_id']==132) & (results_df['y'].isna())]

# df with station 132 and information of true and prediction values
plot_132 = df_132.merge(predict_132, how = 'left', on ='ds')

# prediction v.s. true value plot
plot_132_df = plot_132.set_index('ds')
plot_132_df[['y_x', 'yhat']][0:1000].plot()

import numpy as np
# train RMSE
old_132 = results_df.loc[(results_df['station_id']==132) & (~results_df['y'].isna())]
dif1=np.subtract(np.array(old_132['y']),np.array(old_132['yhat']))
print("Train RMSE = {0}.".format(np.sqrt(np.sum(np.square(dif1))/len(old_132['y']))))
# test RMSE

dif2=np.subtract(np.array(plot_132['y_x']),np.array(plot_132['yhat']))
print("Test RMSE = {0}.".format(np.sqrt(np.sum(np.square(dif2))/len(plot_132['y_x']))))

plt.plot(df_132['ds'],df_132['trend'])
plt.grid(True)
plt.title('Overall Trend', fontsize=14)
plt.xlabel('ds', fontsize=14)
plt.ylabel('trend', fontsize=14)

plt.plot(df_132['yearly'])

# get all results into one df
all_results_df = results_df.merge(sub_df, how = 'right', on =['ds','station_id'])
all_results_df = all_results_df[~all_results_df['yhat'].isna()]

# compute overall rmse
def rmse(x):
    dif1=np.subtract(np.array(x['y_y']),np.array(x['yhat']))
    x['rmse'] = np.sqrt(np.sum(np.square(dif1))/len(x['y_y']))
    return x

rmse_df = all_results_df.groupby('station_id').apply(rmse)

# display overall rmse by each station
(rmse_df.groupby(['station_id','rmse']).size().reset_index()).iloc[:,0:2]



"""## Model"""

# construct cluster_df and reset station_name to index
dfc = pd.read_csv('model_df.csv')  # unzip model_df.zip
cluster_df = dfc.drop_duplicates(subset=['station_name'])

cluster_df.set_index('station_name', inplace = True)
import numpy as np
cluster_df['diff']=np.sign(cluster_df["diff"]).replace([1,-1], [0,1])
cluster_df.rename(columns={"diff": "label"}, inplace = True)
cluster_df.drop(columns = ['start_count', 'end_count','date'], inplace = True)

dfc = dfc.iloc[:,1:37]

cluster_df = spark.createDataFrame(cluster_df)

# create dense feature vetor
features = cluster_df.columns
va = VectorAssembler(inputCols = features, outputCol='features')

va_df = va.transform(cluster_df)
va_df = va_df.select(['features', 'label'])
va_df.show(3)

"""**KNN**"""

!pip3 install KMeans

from pyspark.ml.clustering import KMeans
kmeans = KMeans().setK(16).setSeed(42)
k_model = kmeans.fit(va_df)

KMeans_Assignments=k_model.transform(va_df)

va_df1 = va_df.toPandas()
from sklearn.cluster import MiniBatchKMeans
from sklearn.metrics.pairwise import pairwise_distances_argmin
from sklearn.datasets import make_blobs

knumbers = list(range(5,21))
len(knumbers)

wcss = []
for k in knumbers:
    kmeans = MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',
                init_size=None, max_iter=50, max_no_improvement=10,
                 n_init=3, random_state=542, n_clusters=k,
                reassignment_ratio=0.008, tol=0.0, verbose=0)
    kmeans.fit(va_df1)
    wcss.append(kmeans.inertia_)
plt.style.use("fivethirtyeight")
plt.plot(knumbers, wcss)
plt.xticks(knumbers)
plt.xlabel("Number of Clusters")
plt.ylabel("SSE")
plt.show()

"""**PCA**"""

# plot the clustering result on pricipal component axis
from pyspark.ml.feature import PCA as PCAml
pca = PCAml(k=2, inputCol="features", outputCol="pca")
pca_model = pca.fit(va_df)
pca_transformed = pca_model.transform(va_df)

import numpy as np
x_pca = np.array(pca_transformed.rdd.map(lambda row: row.pca).collect())
cluster_assignment = np.array(KMeans_Assignments.rdd.map(lambda row: row.prediction).collect()).reshape(-1,1)

import seaborn as sns
import matplotlib.pyplot as plt

pca_data = np.hstack((x_pca,cluster_assignment))

pca_df = pd.DataFrame(data=pca_data, columns=("1st_principal", "2nd_principal","cluster_assignment"))
sns.FacetGrid(pca_df,hue="cluster_assignment", height=6).map(plt.scatter, '1st_principal', '2nd_principal' ).add_legend()

plt.show()


# pick up one cluster to analyze 
the_cluster=pca_df.index[pca_df['cluster_assignment'] == 4.0].tolist()
the_cluster

# create subset
sub_cluster_df=dfc[dfc['station_id'].isin(the_cluster)]
sub_cluster_df.shape

sub_df = spark.createDataFrame(sub_cluster_df)

from pyspark.sql.functions import signum
sub_df = sub_df.withColumn("label", signum(sub_df['label']))
newdf = sub_df.replace([-1.0, 1.0], [1, 0], 'label')

from pyspark.sql.types import IntegerType,BooleanType,DateType
tdf=newdf.withColumn("label",newdf.label.cast(IntegerType()))

# assemble features
from pyspark.ml.feature import VectorAssembler
features = tdf.columns[1:]
vc = VectorAssembler(inputCols = features, outputCol='features')

vc_df = vc.transform(tdf)
vc_df = vc_df.select(['features','label'])

"""**Naive Bayes model**"""

# fit the Naive Bayes model
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
(trainset, testset) = vc_df.randomSplit([0.9, 0.1], 480)
#trainset.show()

from pyspark.ml.classification import NaiveBayes
# create the trainer and set its parameters
nb = NaiveBayes(smoothing=1.0, modelType="multinomial")
# train the model
model = nb.fit(trainset)
# select example rows to display.
pred = model.transform(testset)

# evaluate
from sklearn.metrics import confusion_matrix
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator=MulticlassClassificationEvaluator(predictionCol="prediction")
acc = evaluator.evaluate(pred)
print("Prediction Accuracy: ", acc)
y_pred=pred.select("prediction").collect()
y_orig=pred.select("label").collect()
cm = confusion_matrix(y_orig, y_pred)
print("Confusion Matrix:")
print(cm)

"""**CNN**"""

# specify layers for the neural network:
# input layer of size 15 (features), two intermediate of size 8 and 7
# and output of size 2 (classes)
layers = [35, 20, 10, 2]

from pyspark.ml.classification import MultilayerPerceptronClassifier

NN = MultilayerPerceptronClassifier(layers = layers, blockSize = 128).setSeed(42).setMaxIter(100)

# Train the model
model = NN.fit(trainset)

# Make predictions.
predictions = model.transform(testset)
# Select example rows to display
predictions.createOrReplaceTempView("predictions")
spark.sql("SELECT label, prediction FROM predictions").show()

# evaluate
from sklearn.metrics import confusion_matrix
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from sklearn.metrics import recall_score


evaluator = MulticlassClassificationEvaluator(labelCol = 'label', predictionCol = 'prediction', metricName = 'accuracy')
acc = evaluator.evaluate(predictions)
print("Prediction Accuracy: ", acc)

y_pred = predictions.select("prediction").collect()
y_orig = predictions.select("label").collect()

cm = confusion_matrix(y_orig, y_pred)
recall = recall_score(y_orig, y_pred)

print("Testing recall: ", recall)
print("Confusion Matrix:")
print(cm)